{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b9753c9-707d-4e1e-a6b1-3a1c497011b4",
   "metadata": {},
   "source": [
    "# Supervised Image Classification in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950aee00-b546-4686-873d-3a985d4bd0b4",
   "metadata": {},
   "source": [
    "This notebook will walk you through how to do the supervised image classification that was completed in GEM 520 but this time we will complete it Python. It is recommended that you create a new folder with your training polygons and the raster image, as you do not want to overwrite any information from your lab for this process. We will only be going over the scripting part of the lab in this activity to show the differences between R and Python, so it is encouraged that you have your R scripts from that lab open as well to see what the similarities and differences are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c885fb23-08aa-44b1-9718-73e3d374b5c8",
   "metadata": {},
   "source": [
    "We have already installed the correct packages used in this script when you ran the `create_environment.py` script. Now we can import the packages that we will need to run this activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5fa060-9020-43b9-80b3-3bea7d50e42f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import fiona\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from matplotlib.colors import ListedColormap\n",
    "from rasterio.features import geometry_mask\n",
    "from skimage import exposure\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb6248-d936-44b3-bd53-15e0b270fb30",
   "metadata": {},
   "source": [
    "We will start by reading in the Landsat image into Python and plotting it.\n",
    "\n",
    "*Note: This image can also be found [here](https://github.com/ubc-mgem-fcor599/FCOR-599-Workshops/tree/main/CodingCrossover/data) if you are unable to find the image from this assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36308fb-0ff7-45f6-a5da-28a3de645315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in the Landsat image (CHANGE PATH TO YOUR SYSTEM)\n",
    "with rasterio.open(\n",
    "    r\"D:\\MurrayBrent\\git\\CodingCrossover\\data\\LC09_L2SP_047026_20240716_20240717_02_T1_SR_BSTACK.tif\"\n",
    ") as src:\n",
    "    raster = src.read()  # read raster as an array\n",
    "    transform = src.transform  # get the transform information\n",
    "\n",
    "# Create a true colour composite\n",
    "rgb_image = np.stack(\n",
    "    (raster[2], raster[1], raster[0]),\n",
    "    axis=-1,\n",
    ")  # remember that python starts with 0\n",
    "\n",
    "# Plot the true colour composite\n",
    "plt.figure(figsize=(10, 10))  # initalize figure and set size\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.imshow(rgb_image)  # show the true colour composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d35eb-1d84-4ff3-9f36-f3130b76465b",
   "metadata": {},
   "source": [
    "You will notice when we plot this image that it is very dark and that there are some pixels that have a nan value. Below is a function that will implement an image stretch and replace the nan values with 0. We will then plot the true colour composite again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dd830b-77f1-4f90-a8b1-d60734bd7721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def percentile_stretch(band, lower_percentile=2, upper_percentile=98):\n",
    "    band[np.isnan(band)] = 0  # set nan values to 0\n",
    "    band_min, band_max = np.percentile(\n",
    "        band, (lower_percentile, upper_percentile)\n",
    "    )  # get min and max values based on defined percentiles\n",
    "    stretched_band = (band - band_min) / (\n",
    "        band_max - band_min\n",
    "    )  # perform stretch on band\n",
    "    return np.clip(stretched_band, 0, 1)  # return band within value range (0-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec68caf-c988-4d5f-a404-e2607df4b3fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a true colour composite with stretch\n",
    "rgb_image = np.stack(\n",
    "    (\n",
    "        percentile_stretch(raster[2]),\n",
    "        percentile_stretch(raster[1]),\n",
    "        percentile_stretch(raster[0]),\n",
    "    ),\n",
    "    axis=-1,\n",
    ")  # remember that python starts with 0\n",
    "\n",
    "# Plot the true colour composite\n",
    "plt.figure(figsize=(10, 10))  # initalize figure and set size\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.imshow(rgb_image)  # show the true colour composite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef978e9-c47f-4462-b05a-793db9e490f1",
   "metadata": {},
   "source": [
    "Then, we load the delineated polygons and plot them with the image.\n",
    "\n",
    "*Note: A version of these polygons can also be found [here](https://github.com/ubc-mgem-fcor599/FCOR-599-Workshops/tree/main/CodingCrossover/data) if you are unable to find your polygons from the original assignment.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9536b4-eb02-4c84-ae01-6ace1a3bdb98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in classification polygons as geopandas dataframe (CHANGE PATH TO YOUR SYSTEM)\n",
    "with fiona.open(\n",
    "    r\"D:\\MurrayBrent\\git\\CodingCrossover\\data\\classification_polygons.shp\"\n",
    ") as poly:\n",
    "    gdf = gpd.GeoDataFrame.from_features(poly, crs=poly.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88317028-b3e9-4a70-b2c6-204921b7169d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot the image with the polygons\n",
    "fig, ax = plt.subplots(figsize=(10, 10))  # initialize figure\n",
    "ax.imshow(\n",
    "    rgb_image,\n",
    "    extent=(src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top),\n",
    ")  # plot the image\n",
    "gdf.boundary.plot(ax=ax, color=\"red\", linewidth=1)  # plot the boundary of the polygons\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.show()  # show full plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ede7ae-2efb-4084-9e2d-7f8f0a261a12",
   "metadata": {},
   "source": [
    "You will notice when we plotted the polygon boundaries that we called it straight from the `geopandas` dataframe (gdf). Both `geopandas` and `pandas` have built in plotting using `matplotlib`, so we dont need to specifically call any `matplotlib` functions. The functions that are built into `geopandas` and `pandas` use the standard `matplotlib` convention, meaning you can edit and change the plots in the same way you would in `matplotlib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a100ffcc-4ee8-4fd3-b5f1-47731622dc08",
   "metadata": {},
   "source": [
    "Here is a summary of the number of polygons per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71723f4f-0530-4e79-9bcf-7c0ae4af0d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poly_summary = gdf[\"lc_class\"].value_counts()  # get a count of the `lc_class` field\n",
    "poly_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8a6fc2-bf56-4410-9477-7272b87870d8",
   "metadata": {},
   "source": [
    "For each land cover class, we will use 70% of the polygons to train the classification algorithm and the remaining 30% for validation. We are going to add a column to the dataframe called 'set' to identify which polygons will be used for training and which will be used for validation based on a stratified random sample of the 'lc_class' column. We will then split the dataframe into two one for training and one for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1e7ee-6b21-4a8a-ae1d-5fc58a3d6a95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode 'lc_class' so that it is a class number (factor in R)\n",
    "gdf[\"lc_class_encoded\"] = gdf[\"lc_class\"].astype(\"category\").cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f758b81-be55-403c-b2a7-9d5609cc241f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into a 70:30 split\n",
    "train_idx, val_idx = train_test_split(\n",
    "    gdf.index, test_size=0.3, stratify=gdf[\"lc_class\"], random_state=1234\n",
    ")  # stratified random sample based on 'lc_class'\n",
    "\n",
    "# Create new column 'set'\n",
    "gdf[\"set\"] = \"Training\"  # set default value to training\n",
    "gdf.loc[val_idx, \"set\"] = \"Validation\"  # replace value with 'Validation'\n",
    "\n",
    "# Create a Training and a Validation dataframe\n",
    "train_gdf = gdf[gdf[\"set\"] == \"Training\"]  # training dataframe\n",
    "val_gdf = gdf[gdf[\"set\"] == \"Validation\"]  # validation dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60695c5a-cea0-40a9-a5bb-da1680b5b353",
   "metadata": {},
   "source": [
    "We now need to get the raster data ready to perform the maximum likelihood classification. We need to get the shape of the raster, replace any nan values, and flatten the raster for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9f6d2-0831-4f15-8654-cc0bbe8c5d08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess raster for classificaiton\n",
    "(\n",
    "    B,  # band\n",
    "    H,  # height\n",
    "    W,  # width\n",
    ") = raster.shape  # get shape of raster\n",
    "raster = np.nan_to_num(raster, nan=0)  # replace nan to 0\n",
    "flattened_raster = raster.reshape(B, -1).T  # Flatten raster to shape (H*W, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0accb71-6d26-4e04-8ba3-8d9176698cda",
   "metadata": {},
   "source": [
    "Now we can extract the values of the Landsat image based on the training polygons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1538d869-3182-4784-8664-217046135171",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "x_train, y_train = [], []  # empty lists for pixel values and labels\n",
    "for idx, row in train_gdf.iterrows():  # iterate through each polygon\n",
    "    # Generate a mask for the polygon\n",
    "    mask = geometry_mask(\n",
    "        [row.geometry], transform=transform, invert=True, out_shape=(H, W)\n",
    "    )\n",
    "\n",
    "    # Collect pixel values and labels\n",
    "    pixels = flattened_raster[mask.flatten()]  # extract pixel values\n",
    "    labels = np.full(len(pixels), row[\"lc_class_encoded\"])  # get label\n",
    "\n",
    "    # Append to lists\n",
    "    x_train.append(pixels)\n",
    "    y_train.append(labels)\n",
    "\n",
    "# Concatenate training data\n",
    "x_train = np.vstack(x_train)\n",
    "y_train = np.concatenate(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bf8268-3ffe-48ed-b55c-4f2e4272bab2",
   "metadata": {},
   "source": [
    "The maximum likelihood classification will be performed using the `GaussianNB()` function from the `scikit-learn` package. The `GaussianNB()` function implements the Gaussian Naive Bayes algorithm for classification. This function fits to the training data we extracted in the previous step and then predicting the class for each pixel in the raster image. For more information on this function visit the [scikit-learn user guide](https://scikit-learn.org/1.6/modules/generated/sklearn.naive_bayes.GaussianNB.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14077219-8c73-4126-989e-3a3a89923051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train maximum likeihood classifier (Gaussian Naive Bayes)\n",
    "classifier = GaussianNB()  # call model to use\n",
    "classifier.fit(x_train, y_train)  # fit/train model to training data\n",
    "\n",
    "# Classify entier raster\n",
    "predicted = classifier.predict(flattened_raster)  # predict class to each pixel\n",
    "classified_raster = predicted.reshape(\n",
    "    H, W\n",
    ")  # reshape prediction from flattened raster to original dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23341d12-8cfb-466d-8bdf-d4b4127da5d0",
   "metadata": {},
   "source": [
    "Now let's plot the classified raster to see how this model worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0db63a-4eb4-4c9d-913c-0b5bf6d6a009",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set appropriate colours for each class\n",
    "colors = [\"#A6D96A\", \"#33A02C\", \"#DE3B13\", \"#D63CF1\", \"#00D2D2\", \"#F1A026\", \"#2B83BA\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Plot classified image\n",
    "plt.figure(figsize=(10, 10))  # initialize figure and set size\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.imshow(classified_raster, cmap=cmap)  # plot classified raster with defined colours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3736d053-20a6-41a5-ad15-0e39ab8804c5",
   "metadata": {},
   "source": [
    "Using the validation polygons, we can extract the predicted classes and the *'true'* values so we can calculate our accuracy metrics and create a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7a37d-dcd0-4abf-b7e8-60d733503ab4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "true_classes, predicted_classes = [], []  # empty lists for true and predicted classes\n",
    "for idx, row in val_gdf.iterrows():  # iterate through each polygon\n",
    "    # Generate a mask for the polygon\n",
    "    mask = geometry_mask(\n",
    "        [row.geometry], transform=transform, invert=True, out_shape=(H, W)\n",
    "    )\n",
    "\n",
    "    # Extract true clas labels\n",
    "    true_label = row[\"lc_class_encoded\"]  # Column with the true encoded class labels\n",
    "    true_values = np.full(np.sum(mask), true_label)\n",
    "\n",
    "    # Extract predicted class labels from classified raster\n",
    "    predicted_values = classified_raster[mask]\n",
    "\n",
    "    # Append to lists\n",
    "    true_classes.extend(true_values)\n",
    "    predicted_classes.extend(predicted_values)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_classes = np.array(true_classes)\n",
    "predicted_classes = np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f7a12a-aba8-4c86-b903-1780735a57dd",
   "metadata": {},
   "source": [
    "Now we can calculate the overall accuracy and F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e411f904-6f8d-44eb-9b37-49ae04d65c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics and print values\n",
    "oa = accuracy_score(true_classes, predicted_classes)  # overall accuracy\n",
    "f1 = f1_score(true_classes, predicted_classes, average=\"weighted\")  # f1 score\n",
    "print(f\"Overall Accuracy: {oa}\\nF1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd4b7ea-65c9-49cc-9095-0bb5629e6a8b",
   "metadata": {},
   "source": [
    "We can also create a confusion matrix and calculate the Producer's and User's accuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73015b85-d19b-4f10-8dc9-021034f0ecc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Calculate producers and users accuracies from confusion matrix\n",
    "pa = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # producers accuracy\n",
    "ua = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)  # users accuracy\n",
    "\n",
    "# Create dataframe for confusion matrix\n",
    "classes = np.unique(gdf[\"lc_class\"])  # get unique class label\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix.T, index=classes, columns=classes\n",
    ")  # create dataframe\n",
    "\n",
    "# Add producers and users accuracies to dataframe\n",
    "conf_matrix_df.loc[\"Producer's Accuracy\"] = np.round(pa * 100, 3)\n",
    "conf_matrix_df[\"User's Accuracy\"] = np.append(\n",
    "    np.round(ua * 100, 2), [np.nan]\n",
    ")  # Append NaN for alignment\n",
    "\n",
    "# Add titles to show which values are Predicted and which are Actual\n",
    "conf_matrix_df.index.name = \"Predicted\"\n",
    "conf_matrix_df.columns.name = \"Actual\"\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce9a2f-23bd-4c8b-b630-4d29d32a2914",
   "metadata": {},
   "source": [
    "## Comparison to other classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08c33b5-90eb-4b0c-a7fb-b59cc43e17e8",
   "metadata": {},
   "source": [
    "Now that we have the training and validation data created we can run other classifiers to see how they differ. We should use the same data and split as we did previously for a true comparison of these models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857b565-0913-4747-ba85-e44558bcfc0e",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b62f5d26-3f92-4171-8c84-4fbbbdaeaf93",
   "metadata": {},
   "source": [
    "First let's train a Random Forest Classification. Similar to the `GaussianNB()` function we can use the `RandomForestClassifier()` function from `scikit-learn` ([User Guide](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).\n",
    "\n",
    "In this example we will have 100 'trees' in our forest but feel free to adjust this parameter to see how the output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd42b1-4df2-4098-b7dc-fc69f08650f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, random_state=42\n",
    ")  # call model to use and set parameters\n",
    "rf_classifier.fit(x_train, y_train)  # fit/train model to training data\n",
    "\n",
    "# Classify entier raster\n",
    "predicted = rf_classifier.predict(flattened_raster)  # predict class to each pixel\n",
    "classified_raster = predicted.reshape(\n",
    "    H, W\n",
    ")  # reshape prediction from flattened raster to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c544be7-4617-4f21-9e5c-011beefe7aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set appropriate colours for each class\n",
    "colors = [\"#A6D96A\", \"#33A02C\", \"#DE3B13\", \"#D63CF1\", \"#00D2D2\", \"#F1A026\", \"#2B83BA\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Plot classified image\n",
    "plt.figure(figsize=(10, 10))  # initialize figure and set size\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.imshow(classified_raster, cmap=cmap)  # plot classified raster with defined colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29a4fee-8e98-4cf5-8cbb-31c3191e652c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "true_classes, predicted_classes = [], []  # empty lists for true and predicted classes\n",
    "for idx, row in val_gdf.iterrows():  # iterate through each polygon\n",
    "    # Generate a mask for the polygon\n",
    "    mask = geometry_mask(\n",
    "        [row.geometry], transform=transform, invert=True, out_shape=(H, W)\n",
    "    )\n",
    "\n",
    "    # Extract true clas labels\n",
    "    true_label = row[\"lc_class_encoded\"]  # Column with the true encoded class labels\n",
    "    true_values = np.full(np.sum(mask), true_label)\n",
    "\n",
    "    # Extract predicted class labels from classified raster\n",
    "    predicted_values = classified_raster[mask]\n",
    "\n",
    "    # Append to lists\n",
    "    true_classes.extend(true_values)\n",
    "    predicted_classes.extend(predicted_values)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_classes = np.array(true_classes)\n",
    "predicted_classes = np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd66df8-166e-4967-9a9f-78a95d5ee439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics and print values\n",
    "oa = accuracy_score(true_classes, predicted_classes)  # overall accuracy\n",
    "f1 = f1_score(true_classes, predicted_classes, average=\"weighted\")  # f1 score\n",
    "print(f\"Overall Accuracy: {oa}\\nF1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fa72a-4b4c-48b1-8f7e-90ff02921dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Calculate producers and users accuracies from confusion matrix\n",
    "pa = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # producers accuracy\n",
    "ua = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)  # users accuracy\n",
    "\n",
    "# Create dataframe for confusion matrix\n",
    "classes = np.unique(gdf[\"lc_class\"])  # get unique class label\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix.T, index=classes, columns=classes\n",
    ")  # create dataframe\n",
    "\n",
    "# Add producers and users accuracies to dataframe\n",
    "conf_matrix_df.loc[\"Producer's Accuracy\"] = np.round(pa * 100, 3)\n",
    "conf_matrix_df[\"User's Accuracy\"] = np.append(\n",
    "    np.round(ua * 100, 2), [np.nan]\n",
    ")  # Append NaN for alignment\n",
    "\n",
    "# Add titles to show which values are Predicted and which are Actual\n",
    "conf_matrix_df.index.name = \"Predicted\"\n",
    "conf_matrix_df.columns.name = \"Actual\"\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d8742-6c10-4ea3-b77e-0a49d6581750",
   "metadata": {},
   "source": [
    "#### Random Forest - Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a65084-b70f-48f0-9a4c-2c602ec161e7",
   "metadata": {},
   "source": [
    "An additional analysis we can do with Random Forest is see the feature importance of the different inputs for the classification. This allows us to further understand why our outputs might be influenced by our inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672fafd-3f9b-4aec-8253-8144b6b1560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = rf_classifier.feature_importances_\n",
    "bands = [\"Blue\", \"Green\", \"Red\", \"NIR\", \"SWIR1\", \"SWIR2\"]\n",
    "for band, importance in zip(bands, feature_importance):\n",
    "    print(f\"{band}: {importance}\")\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(bands, feature_importance, color=\"orange\")\n",
    "plt.xlabel(\"Feature Importance\")\n",
    "plt.ylabel(\"Band\")\n",
    "plt.title(\"Feature Importance in Random Forest\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6364192a-6c48-42bd-8919-b2e987d651bf",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e30e0-acb6-45f9-892a-d2b8d3192a2e",
   "metadata": {},
   "source": [
    "Now lets train a Support Vector Machine (SVM) using the `SVC()` function ([User Guide](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c962292c-c33e-4c2b-b68e-f30c8c7913dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Support Vector Machine (SVM) Classifier\n",
    "svm_classifier = SVC(\n",
    "    kernel=\"linear\", C=1.0, random_state=42\n",
    ")  # call model to use and set parameters\n",
    "svm_classifier.fit(x_train, y_train)  # fit/train model to training data\n",
    "\n",
    "# Classify entier raster\n",
    "predicted = svm_classifier.predict(flattened_raster)  # predict class to each pixel\n",
    "classified_raster = predicted.reshape(\n",
    "    H, W\n",
    ")  # reshape prediction from flattened raster to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c88765-41bf-4e8a-a1f3-1065b06ba51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set appropriate colours for each class\n",
    "colors = [\"#A6D96A\", \"#33A02C\", \"#DE3B13\", \"#D63CF1\", \"#00D2D2\", \"#F1A026\", \"#2B83BA\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Plot classified image\n",
    "plt.figure(figsize=(10, 10))  # initialize figure and set size\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.imshow(classified_raster, cmap=cmap)  # plot classified raster with defined colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43da1af3-59dd-48b8-8804-3239c134ebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "true_classes, predicted_classes = [], []  # empty lists for true and predicted classes\n",
    "for idx, row in val_gdf.iterrows():  # iterate through each polygon\n",
    "    # Generate a mask for the polygon\n",
    "    mask = geometry_mask(\n",
    "        [row.geometry], transform=transform, invert=True, out_shape=(H, W)\n",
    "    )\n",
    "\n",
    "    # Extract true clas labels\n",
    "    true_label = row[\"lc_class_encoded\"]  # Column with the true encoded class labels\n",
    "    true_values = np.full(np.sum(mask), true_label)\n",
    "\n",
    "    # Extract predicted class labels from classified raster\n",
    "    predicted_values = classified_raster[mask]\n",
    "\n",
    "    # Append to lists\n",
    "    true_classes.extend(true_values)\n",
    "    predicted_classes.extend(predicted_values)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_classes = np.array(true_classes)\n",
    "predicted_classes = np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4353227-215a-4490-9f7c-092c825a8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics and print values\n",
    "oa = accuracy_score(true_classes, predicted_classes)  # overall accuracy\n",
    "f1 = f1_score(true_classes, predicted_classes, average=\"weighted\")  # f1 score\n",
    "print(f\"Overall Accuracy: {oa}\\nF1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4dddd2-a338-4e15-8e05-c4fd96aa3632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Calculate producers and users accuracies from confusion matrix\n",
    "pa = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # producers accuracy\n",
    "ua = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)  # users accuracy\n",
    "\n",
    "# Create dataframe for confusion matrix\n",
    "classes = np.unique(gdf[\"lc_class\"])  # get unique class label\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix.T, index=classes, columns=classes\n",
    ")  # create dataframe\n",
    "\n",
    "# Add producers and users accuracies to dataframe\n",
    "conf_matrix_df.loc[\"Producer's Accuracy\"] = np.round(pa * 100, 3)\n",
    "conf_matrix_df[\"User's Accuracy\"] = np.append(\n",
    "    np.round(ua * 100, 2), [np.nan]\n",
    ")  # Append NaN for alignment\n",
    "\n",
    "# Add titles to show which values are Predicted and which are Actual\n",
    "conf_matrix_df.index.name = \"Predicted\"\n",
    "conf_matrix_df.columns.name = \"Actual\"\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa4fb8e-1c80-4afc-9c4e-9621dc6fc285",
   "metadata": {},
   "source": [
    "### Neural Networks (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ebfa2-9bdd-449e-b4e8-a70ce3ecd3d1",
   "metadata": {},
   "source": [
    "Finally, let's train a simple neural network to compare the other classification outputs. This neural network is a Multi-Layer Perceptron with three hidden layers containing 100, 50, and 25 neurons. Feel free to adjust the number of layers as well as the number of neurons to see how this changes the output. For more information on the `MLPClassifier()` function see the [User Guide](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b8528-d729-426f-a9cc-6e4859a8c176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multi-Layer Perceptron Classifier\n",
    "mlp_classifier = MLPClassifier(\n",
    "    hidden_layer_sizes=(\n",
    "        100,\n",
    "        50,\n",
    "        25,\n",
    "    ),  # Three hidden layer with 100, 50, and 25 neurons\n",
    "    activation=\"relu\",  # Activation function for hidden layers\n",
    "    solver=\"adam\",  # Optimization algorithm\n",
    "    max_iter=300,  # Maximum number of iterations\n",
    "    random_state=42,\n",
    ")\n",
    "mlp_classifier.fit(x_train, y_train)  # fit/train model to training data\n",
    "\n",
    "# Classify entier raster\n",
    "predicted = mlp_classifier.predict(flattened_raster)  # predict class to each pixel\n",
    "classified_raster = predicted.reshape(\n",
    "    H, W\n",
    ")  # reshape prediction from flattened raster to original dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73cc0cc-91fe-460f-9248-3556c27073b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set appropriate colours for each class\n",
    "colors = [\"#A6D96A\", \"#33A02C\", \"#DE3B13\", \"#D63CF1\", \"#00D2D2\", \"#F1A026\", \"#2B83BA\"]\n",
    "cmap = ListedColormap(colors)\n",
    "\n",
    "# Plot classified image\n",
    "plt.figure(figsize=(10, 10))  # initialize figure and set size\n",
    "plt.axis(\"off\")  # turn axis off for visualization\n",
    "plt.imshow(classified_raster, cmap=cmap)  # plot classified raster with defined colours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017ffd3-d4b2-490e-bcba-bbe7e24a6dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare validation data\n",
    "true_classes, predicted_classes = [], []  # empty lists for true and predicted classes\n",
    "for idx, row in val_gdf.iterrows():  # iterate through each polygon\n",
    "    # Generate a mask for the polygon\n",
    "    mask = geometry_mask(\n",
    "        [row.geometry], transform=transform, invert=True, out_shape=(H, W)\n",
    "    )\n",
    "\n",
    "    # Extract true clas labels\n",
    "    true_label = row[\"lc_class_encoded\"]  # Column with the true encoded class labels\n",
    "    true_values = np.full(np.sum(mask), true_label)\n",
    "\n",
    "    # Extract predicted class labels from classified raster\n",
    "    predicted_values = classified_raster[mask]\n",
    "\n",
    "    # Append to lists\n",
    "    true_classes.extend(true_values)\n",
    "    predicted_classes.extend(predicted_values)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "true_classes = np.array(true_classes)\n",
    "predicted_classes = np.array(predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232761b3-fb86-4bc5-9ba7-f91eff3819e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy metrics and print values\n",
    "oa = accuracy_score(true_classes, predicted_classes)  # overall accuracy\n",
    "f1 = f1_score(true_classes, predicted_classes, average=\"weighted\")  # f1 score\n",
    "print(f\"Overall Accuracy: {oa}\\nF1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa997bc0-2220-497f-8d2e-e8daa4d5e5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix\n",
    "conf_matrix = confusion_matrix(true_classes, predicted_classes)\n",
    "\n",
    "# Calculate producers and users accuracies from confusion matrix\n",
    "pa = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)  # producers accuracy\n",
    "ua = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)  # users accuracy\n",
    "\n",
    "# Create dataframe for confusion matrix\n",
    "classes = np.unique(gdf[\"lc_class\"])  # get unique class label\n",
    "conf_matrix_df = pd.DataFrame(\n",
    "    conf_matrix.T, index=classes, columns=classes\n",
    ")  # create dataframe\n",
    "\n",
    "# Add producers and users accuracies to dataframe\n",
    "conf_matrix_df.loc[\"Producer's Accuracy\"] = np.round(pa * 100, 3)\n",
    "conf_matrix_df[\"User's Accuracy\"] = np.append(\n",
    "    np.round(ua * 100, 2), [np.nan]\n",
    ")  # Append NaN for alignment\n",
    "\n",
    "# Add titles to show which values are Predicted and which are Actual\n",
    "conf_matrix_df.index.name = \"Predicted\"\n",
    "conf_matrix_df.columns.name = \"Actual\"\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d952ad8-96f8-46e9-86b6-deae989aee1c",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd7c84-4cd1-4a41-888a-b536df205b90",
   "metadata": {},
   "source": [
    "Question 1 - Why is it important to use the same training and validation data when comparing different model outputs?\n",
    "\n",
    "Question 2 - Which classifier performed best with your data? Why do you think this is?\n",
    "\n",
    "Screenshot - Save a screenshot of your best model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_new_environment)",
   "language": "python",
   "name": "my_new_environment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
